{
 "metadata": {
  "name": "",
  "signature": "sha256:b54211c0edaa4ea06f1b2f5ad3d659fbbe62d74df198ed19c5ad9625b88a3414"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyfiglet\n",
      "phrase = 'Oauth2'\n",
      "print \"x = \\\"\\\"\\\"\"\n",
      "print pyfiglet.figlet_format(phrase, font='standard')\n",
      "print \"    \\\"\\\"\\\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = \"\"\"\n",
        "  ___              _   _     ____  \n",
        " / _ \\  __ _ _   _| |_| |__ |___ \\ \n",
        "| | | |/ _` | | | | __| '_ \\  __) |\n",
        "| |_| | (_| | |_| | |_| | | |/ __/ \n",
        " \\___/ \\__,_|\\__,_|\\__|_| |_|_____|\n",
        "                                   \n",
        "\n",
        "    \"\"\"\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Download from list of #urls\n",
      "\n",
      "# import urllib\n",
      "# import urllib2\n",
      "# import requests\n",
      " \n",
      "# url = 'http://www.blog.pythonlibrary.org/wp-content/uploads/2012/06/wxDbViewer.zip'\n",
      " \n",
      "# print \"downloading with urllib\"\n",
      "# urllib.urlretrieve(url, \"code.zip\")\n",
      " \n",
      "# print \"downloading with urllib2\"\n",
      "# f = urllib2.urlopen(url)\n",
      "# data = f.read()\n",
      "# with open(\"code2.zip\", \"wb\") as code:\n",
      "#     code.write(data)\n",
      " \n",
      "# print \"downloading with requests\"\n",
      "# r = requests.get(url)\n",
      "# with open(\"code3.zip\", \"wb\") as code:\n",
      "#     code.write(r.content)\n",
      "    \n",
      "###\n",
      "\n",
      "import urllib\n",
      "\n",
      "urllist = ['http://www.oshannonland.com/wp-content/uploads/2012/06/20-Spider-Man-20.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/21-Spider-Man-21.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/22-Spider-Man-22.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/23-Spider-Man-231.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/24-Spider-Man-24.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/25-Spider-Man-25.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/26-Spider-Man-26.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-27.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-28.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-29.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-30.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-31.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-32.mp3', \n",
      "'http://www.oshannonland.com/wp-content/uploads/2012/06/spider-man-33.mp3']\n",
      "\n",
      "for url in urllist:\n",
      "    urllib.urlretrieve(url, url.rsplit('/', 1)[-1])\n",
      "    print url.rsplit('/', 1)[-1] + ' downloaded successfully'\n",
      "    \n",
      "###\n",
      "\n",
      "# from text file:\n",
      "\n",
      "download_full_path = 'c:/eraseme/'\n",
      "urlfile_full_path = 'c:/eraseme/urls.txt'\n",
      "overwrite = False\n",
      "\n",
      "import os\n",
      "import urllib\n",
      "\n",
      "os.chdir(download_full_path)\n",
      "\n",
      "# For every line in the file\n",
      "errors=[]\n",
      "for url in open(urlfile_full_path):\n",
      "    try:# Split on the rightmost / and take everything on the right side of that\n",
      "        name = url.rsplit('/', 1)[-1]\n",
      "    \n",
      "        # Combine the name and the downloads directory to get the local filename\n",
      "        filename = os.path.join(download_full_path, name)\n",
      "    \n",
      "        # Download the file if it does not exist\n",
      "        if not os.path.isfile(filename) or overwrite == True:\n",
      "            urllib.urlretrieve(url, filename)\n",
      "            print filename\n",
      "    except:\n",
      "        errors.append(url)\n",
      "print \"Errors:\",\n",
      "for url in errors:\n",
      "    print url,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = \"\"\"\n",
      " _           _ _          _    ___              _        __                       _        _      __ _ _      \n",
      "| |__  _   _| | | __   __| |  / / |  _   _ _ __| |___   / _|_ __ ___  _ __ ___   | |___  _| |_   / _(_) | ___ \n",
      "| '_ \\| | | | | |/ /  / _` | / /| | | | | | '__| / __| | |_| '__/ _ \\| '_ ` _ \\  | __\\ \\/ / __| | |_| | |/ _ \\\n",
      "| |_) | |_| | |   <  | (_| |/ / | | | |_| | |  | \\__ \\ |  _| | | (_) | | | | | | | |_ >  <| |_  |  _| | |  __/\n",
      "|_.__/ \\__,_|_|_|\\_\\  \\__,_/_/  |_|  \\__,_|_|  |_|___/ |_| |_|  \\___/|_| |_| |_|  \\__/_/\\_\\\\__| |_| |_|_|\\___|\n",
      "                                                                                                              \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "download_full_path = 'c:/eraseme/'\n",
      "urlfile_full_path = 'c:/eraseme/urls.txt'\n",
      "overwrite = False\n",
      "\n",
      "import os\n",
      "import urllib\n",
      "\n",
      "os.chdir(download_full_path)\n",
      "\n",
      "# For every line in the file\n",
      "errors=[]\n",
      "for url in open(urlfile_full_path):\n",
      "    try:# Split on the rightmost / and take everything on the right side of that\n",
      "        name = url.rsplit('/', 1)[-1]\n",
      "    \n",
      "        # Combine the name and the downloads directory to get the local filename\n",
      "        filename = os.path.join(download_full_path, name)\n",
      "    \n",
      "        # Download the file if it does not exist\n",
      "        if not os.path.isfile(filename) or overwrite == True:\n",
      "            urllib.urlretrieve(url, filename)\n",
      "            print filename\n",
      "    except:\n",
      "        errors.append(url)\n",
      "print \"Errors:\",\n",
      "for url in errors:\n",
      "    print url,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "x = \"\"\"\n",
      "  ___              _   _     ____  \n",
      " / _ \\  __ _ _   _| |_| |__ |___ \\ \n",
      "| | | |/ _` | | | | __| '_ \\  __) |\n",
      "| |_| | (_| | |_| | |_| | | |/ __/ \n",
      " \\___/ \\__,_|\\__,_|\\__|_| |_|_____|\n",
      "                                   \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "#oauth2\n",
      "\n",
      "import urlparse\n",
      "import oauth2 as oauth\n",
      "\n",
      "consumer_key = 'azkeVAunE4IK2ChXoGifruSQ4YpxLwafX1dLZrLAkzLJrc2IcE'\n",
      "consumer_secret = 'DhQLUrawaPosoCaMmrSTMyvBfQjaiSUQGV1mJ9vW9yvmu2GURB'\n",
      "\n",
      "request_token_url = 'http://www.tumblr.com/oauth/request_token'\n",
      "access_token_url = 'http://www.tumblr.com/oauth/access_token'\n",
      "authorize_url = 'http://www.tumblr.com/oauth/authorize'\n",
      "\n",
      "consumer = oauth.Consumer(consumer_key, consumer_secret)\n",
      "client = oauth.Client(consumer)\n",
      "\n",
      "# Step 1: Get a request token. This is a temporary token that is used for \n",
      "# having the user authorize an access token and to sign the request to obtain \n",
      "# said access token.\n",
      "\n",
      "resp, content = client.request(request_token_url, \"GET\")\n",
      "if resp['status'] != '200':\n",
      "    raise Exception(\"Invalid response %s.\" % resp['status'])\n",
      "\n",
      "request_token = dict(urlparse.parse_qsl(content))\n",
      "\n",
      "print \"Request Token:\"\n",
      "print \"    - oauth_token        = %s\" % request_token['oauth_token']\n",
      "print \"    - oauth_token_secret = %s\" % request_token['oauth_token_secret']\n",
      "print \n",
      "\n",
      "# Step 2: Redirect to the provider. Since this is a CLI script we do not \n",
      "# redirect. In a web application you would redirect the user to the URL\n",
      "# below.\n",
      "\n",
      "print \"Go to the following link in your browser:\"\n",
      "print \"%s?oauth_token=%s\" % (authorize_url, request_token['oauth_token'])\n",
      "print \n",
      "\n",
      "# After the user has granted access to you, the consumer, the provider will\n",
      "# redirect you to whatever URL you have told them to redirect to. You can \n",
      "# usually define this in the oauth_callback argument as well.\n",
      "accepted = 'n'\n",
      "while accepted.lower() == 'n':\n",
      "    accepted = raw_input('Have you authorized me? (y/n) ')\n",
      "oauth_verifier = raw_input('What is the PIN? ')\n",
      "\n",
      "# Step 3: Once the consumer has redirected the user back to the oauth_callback\n",
      "# URL you can request the access token the user has approved. You use the \n",
      "# request token to sign this request. After this is done you throw away the\n",
      "# request token and use the access token returned. You should store this \n",
      "# access token somewhere safe, like a database, for future use.\n",
      "token = oauth.Token(request_token['oauth_token'],\n",
      "    request_token['oauth_token_secret'])\n",
      "token.set_verifier(oauth_verifier)\n",
      "client = oauth.Client(consumer, token)\n",
      "\n",
      "resp, content = client.request(access_token_url, \"POST\")\n",
      "access_token = dict(urlparse.parse_qsl(content))\n",
      "\n",
      "print \"Access Token:\"\n",
      "print \"    - oauth_token        = %s\" % access_token['oauth_token']\n",
      "print \"    - oauth_token_secret = %s\" % access_token['oauth_token_secret']\n",
      "print\n",
      "print \"You may now access protected resources using the access tokens above.\" \n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = \"\"\"\n",
      " ____  _____ _____ ____  \n",
      "/ ___||  ___|_   _|  _ \\ \n",
      "\\___ \\| |_    | | | |_) |\n",
      " ___) |  _|   | | |  __/ \n",
      "|____/|_|     |_| |_|    \n",
      "                         \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "#SFTP transfer\n",
      "\n",
      "# Transfer local to #ftp\n",
      "\n",
      "import pysftp\n",
      "sftpconn = pysftp.Connection('ssh.REDACTED.com', \n",
      "                                          username='REDACTED', password='REDACTED$)\n",
      "sftpconn.put('C:/Users/David/Documents/Images_API/sked/sched.json', 'sked/sched.json')\n",
      "\n",
      "# # Transfer ftp to local\n",
      "\n",
      "import pysftp\n",
      "sftpconn = pysftp.Connection('ssh.REDACTED.com', \n",
      "                                          username='REDACTED', password='REDACTED')\n",
      "sftpconn.get('sked/sched.json', 'C:/Users/David/Documents/Images_API/sked/sched.json')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#BeautifulSoup\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(html_string) #or soup = BeautifulSoup(open(\"index.html\"))\n",
      "\n",
      "#to find first such tag:\n",
      "tag = soup.find('p')\n",
      "#attributes:\n",
      "tag.name\n",
      "tag['class'] # an attr\n",
      "tag.contents\n",
      "tag.attrs # returns a dict\n",
      "\n",
      "#to find all tags (returns list)\n",
      "tags = soup.findAll('p')\n",
      "\n",
      "#to find <select name=\"HSpeaker\" multiple size=\"4\">\n",
      "hspeaker = soup.find('select',{'name':'HSpeaker'})\n",
      "\n",
      "#to create list all option tags under the above:\n",
      "options = hspeaker.findAll('option')\n",
      "\n",
      "print soup.find(id=\"link3\")\n",
      "\n",
      "#extract all urls in <a href=\"\"> tags\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.get('href'))\n",
      "    \n",
      "#extract all text\n",
      "print(soup.get_text())\n",
      "\n",
      "# more:\n",
      "# http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mechanize\n",
      "\n",
      "import mechanize\n",
      "\n",
      "br = mechanize.Browser()\n",
      "br.set_handle_robots(False)   # ignore robots          # no cookies\n",
      "br.set_handle_equiv(True)\n",
      "br.set_handle_redirect(True)\n",
      "br.set_handle_referer(True)\n",
      "# Follows refresh 0 but not hangs on refresh > 0:\n",
      "br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n",
      "\n",
      "user_agents = ['Mozilla/5.0 (Windows; U; Windows NT 5.1; en-GB; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6',\n",
      "               'Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.472.63 Safari/534.3',\n",
      "               'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
      "               'Opera/9.00 (Windows NT 5.1; U; en)']\n",
      "\n",
      "agent = 2\n",
      "\n",
      "br.addheaders = [('User-agent', user_agents[agent])]\n",
      "\n",
      "# Want debugging messages?\n",
      "#br.set_debug_http(True)\n",
      "#br.set_debug_redirects(True)\n",
      "#br.set_debug_responses(True)\n",
      "\n",
      "#alternate to use cookies\n",
      "# import cookielib\n",
      "# cj = cookielib.LWPCookieJar()\n",
      "# br.set_cookiejar(cj)\n",
      "\n",
      "#########################################\n",
      "\n",
      "response = br.open('http://www.example.com') # can only be used once\n",
      "soup = BeautifulSoup(response.read())\n",
      "assert response.code == 200 # headers\n",
      "\n",
      "#forms\n",
      "for form in br.forms():\n",
      "    print form\n",
      "br.select_form(nr=0) #first form\n",
      "br.select_form(\"form1\")         # works when form has a name\n",
      "br.form = list(br.forms())[0]  # use when form is unnamed\n",
      "\n",
      "# submit form\n",
      "# eg if form control is <CheckboxControl(SenateSection=[*1])>\n",
      "br.form['SenateSection']=1\n",
      "br.submit()\n",
      "print br.response().read()\n",
      "#list all controls\n",
      "for control in br.form.controls:\n",
      "    print control\n",
      "    print \"type=%s, name=%s value=%s\" % (control.type, control.name, br[control.name])\n",
      "#find by name\n",
      "control = br.form.find_control(\"controlname\")\n",
      "#allowed values\n",
      "if control.type == \"select\":  # means it's class ClientForm.SelectControl\n",
      "    for item in control.items:\n",
      "    print \" name=%s values=%s\" % (item.name, str([label.text  for label in item.get_labels()]))\n",
      "#select-type controls must be set with a list, even if only one item:\n",
      "print control.value\n",
      "print control  # selected value is starred\n",
      "control.value = [\"ItemName\"]\n",
      "print control\n",
      "br[control.name] = [\"ItemName\"]  # equivalent and more normal\n",
      "#text controls can be set as string\n",
      "if control.type == \"text\":  # means it's class ClientForm.TextControl\n",
      "    control.value = \"stuff here\"\n",
      "br[\"controlname\"] = \"stuff here\"  # equivalent\n",
      "    \n",
      "    \n",
      "\n",
      "# Looking at some results in link format\n",
      "for link in br.links(url_regex='stockrt'):\n",
      "    print link\n",
      "    #or\n",
      "    print link.text, link.url\n",
      "\n",
      "#links\n",
      "# Testing presence of link (if the link is not found you would have to\n",
      "# handle a LinkNotFoundError exception)\n",
      "br.find_link(text='Weekend codes')\n",
      "# Actually clicking the link\n",
      "req = br.click_link(text='Weekend codes')\n",
      "br.open(req)\n",
      "print br.response().read()\n",
      "print br.geturl()\n",
      "# Back\n",
      "br.back()\n",
      "print br.response().read()\n",
      "print br.geturl()\n",
      "#\n",
      "br.follow_link(text='Sign out')\n",
      "#or\n",
      "resp = br.follow_link(...)\n",
      "# follow lots of links\n",
      "all_links = [l for l in br.links(url_regex='\\?v=c&th=')] # or text_regex\n",
      "# Select the first 3\n",
      "for link in all_links[0:3]:\n",
      "    print link\n",
      "# Open each message\n",
      "br.follow_link(msg_link)\n",
      "#\n",
      "request = br.click_link(link)\n",
      "response = br.follow_link(link)\n",
      "print response.geturl()\n",
      "print response.get_data()\n",
      "#\n",
      "for link in br.links():\n",
      "    print link.text, link.url\n",
      "\n",
      "# Download\n",
      "f = br.retrieve('http://www.google.com.br/intl/pt-BR_br/images/logo.gif')[0]\n",
      "\n",
      "# Proxy and user/password\n",
      "br.set_proxies({\"http\": \"joe:password@myproxy.example.com:3128\"})\n",
      "# Proxy\n",
      "br.set_proxies({\"http\": \"myproxy.example.com:3128\"})\n",
      "# Proxy password\n",
      "br.add_proxy_password(\"joe\", \"password\")\n",
      "\n",
      "# more, incl password protected auth\n",
      "# http://stockrt.github.io/p/emulating-a-browser-in-python-with-mechanize/\n",
      "\n",
      "#pydoc\n",
      "# http://joesourcecode.com/Documentation/mechanize0.2.5/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}